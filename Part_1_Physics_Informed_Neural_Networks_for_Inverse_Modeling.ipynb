{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMves9lMoBdK"
      },
      "source": [
        "# Part 1 — Inverse Modeling with PINNs\n",
        "\n",
        "In this notebook we learn how to **estimate an unknown physical parameter** from limited data **using physics-informed neural networks**.\n",
        "\n",
        "We revisit our familiar **1D damped harmonic oscillator**:\n",
        "\n",
        "$$\n",
        "m\\frac{d^2x}{dt^2} + \\mu\\frac{dx}{dt} + kx = 0,\n",
        "$$\n",
        "with the initial conditions\n",
        "$$\n",
        "x(0) = 1 \\, , \\quad \\frac{dx}{dt}\\bigg\\rvert_{t=0} = 0.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "This time, $m$ (the *mass*) is *unknown*.\n",
        "We'll train a PINN to **infer \\(m\\)** while fitting the data and satisfying the differential equation.\n",
        "This notebook is inspired by the [blog post of Ben Moseley](https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUJugSLwoVvd"
      },
      "outputs": [],
      "source": [
        "#@title Install & imports (run first)\n",
        "!pip install -q torch torchvision matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0fFY2SVnzsQ",
        "outputId": "ed2a8f8f-4570-47f3-f1a5-bda2a7080cec"
      },
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84rwiTpyonzf"
      },
      "source": [
        "## Step 1 — Generate synthetic data\n",
        "\n",
        "We'll use the *analytical solution* of the underdamped oscillator to produce data with **true parameters**:\n",
        "- $m_\\text{true} = 1.5$\n",
        "- $\\mu = 0.3$\n",
        "- $k = 4.0$\n",
        "\n",
        "We'll only provide the network with a few early-time data points. It must infer $m$ from those and from the physics constraint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "9BphBExuolah",
        "outputId": "449b66c0-9f9c-4765-d8fd-14f5f789c496"
      },
      "outputs": [],
      "source": [
        "# Ground truth generator\n",
        "def oscillator_gt(t, m=1.5, mu=0.3, k=4.0, x0=1.0, v0=0.0):\n",
        "    t = np.asarray(t, dtype=float)\n",
        "    gamma = mu / m\n",
        "    omega0 = np.sqrt(k / m)\n",
        "    disc = omega0**2 - (gamma**2) / 4.0\n",
        "    if disc > 0:\n",
        "        omega_d = np.sqrt(disc)\n",
        "        A = x0\n",
        "        B = (v0 + (gamma/2.0)*A) / omega_d\n",
        "        x = np.exp(-(gamma/2.0)*t) * (A*np.cos(omega_d*t) + B*np.sin(omega_d*t))\n",
        "    else:\n",
        "        x = np.exp(-(gamma/2.0)*t) * (x0 + (v0 + (gamma/2.0)*x0)*t)\n",
        "    return x\n",
        "\n",
        "# True parameters\n",
        "m_true, mu, k = 1.5, 0.3, 4.0\n",
        "x0_val, v0_val = 1.0, 0.0\n",
        "\n",
        "# Time domain\n",
        "t0, t_max_data, t_max_model = 0.0, 3.0, 8.0\n",
        "t_all = np.linspace(t0, t_max_model, 400)\n",
        "x_all = oscillator_gt(t_all, m_true, mu, k, x0_val, v0_val)\n",
        "\n",
        "# Sparse training data\n",
        "N_data = 18\n",
        "t_train = np.sort(np.random.RandomState(42).uniform(t0, t_max_data, N_data))\n",
        "x_train = oscillator_gt(t_train, m_true, mu, k, x0_val, v0_val)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(t_all, x_all, label=\"Ground truth\", linewidth=2)\n",
        "plt.scatter(t_train, x_train, color='tab:orange', label=\"Observed data\", zorder=5)\n",
        "plt.xlabel(\"t\")\n",
        "plt.ylabel(\"x(t)\")\n",
        "plt.title(\"Damped Harmonic Oscillator (true mass m=1.5)\")\n",
        "plt.legend(); plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0eb2ZGhp0rV"
      },
      "source": [
        "## Step 2 — Define the Neural Network Model\n",
        "\n",
        "We'll use a simple **fully connected network** that takes time $t$ as input and predicts $x(t)$.\n",
        "\n",
        "We also introduce a **trainable parameter** $\\log m$ so that $m = e^{\\log m}$ stays positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFwdvMquqDsa"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, layers, activation=nn.Tanh):\n",
        "        super().__init__()\n",
        "        mods = []\n",
        "        for i in range(len(layers)-1):\n",
        "            mods.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            if i < len(layers)-2:\n",
        "                mods.append(activation())\n",
        "        self.net = nn.Sequential(*mods)\n",
        "        # Xavier init\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "    def forward(self, t):\n",
        "        return self.net(t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewxHi6lHpjtF"
      },
      "source": [
        "## Step 3 — Helper: Derivatives with autograd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MNXLQtjo6an"
      },
      "outputs": [],
      "source": [
        "def time_derivative(u, t, order=1):\n",
        "    deriv = u\n",
        "    for _ in range(order):\n",
        "        deriv = torch.autograd.grad(\n",
        "            outputs=deriv,\n",
        "            inputs=t,\n",
        "            grad_outputs=torch.ones_like(deriv),\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "        )[0]\n",
        "    return deriv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SayWu3pprnW"
      },
      "source": [
        "## Step 4 — Prepare tensors and instantiate model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTVYgoSmpvcB",
        "outputId": "0a62ee18-92b9-4625-f3e8-e2c2b61946cf"
      },
      "outputs": [],
      "source": [
        "# Convert data to tensors\n",
        "t_train_t = torch.tensor(t_train.reshape(-1,1), dtype=torch.float32, device=device)\n",
        "x_train_t = torch.tensor(x_train.reshape(-1,1), dtype=torch.float32, device=device)\n",
        "\n",
        "# Collocation points (physics)\n",
        "N_collocation = 400\n",
        "t_collocation_t = torch.tensor(\n",
        "    np.random.RandomState(0).uniform(t0, t_max_model, N_collocation).reshape(-1,1),\n",
        "    dtype=torch.float32, device=device, requires_grad=True)\n",
        "\n",
        "# Initial conditions\n",
        "t0_t = torch.tensor([[0.0]], dtype=torch.float32, device=device, requires_grad=True)\n",
        "x0_t = torch.tensor([[x0_val]], dtype=torch.float32, device=device)\n",
        "v0_t = torch.tensor([[v0_val]], dtype=torch.float32, device=device)\n",
        "\n",
        "# Instantiate network and trainable parameter\n",
        "model = MLP([1,128,128,128,1], activation=nn.Tanh).to(device)\n",
        "log_m = torch.nn.Parameter(torch.tensor([np.log(1.0)], dtype=torch.float32, device=device, requires_grad=True))\n",
        "print(\"Initial mass guess m ≈\", float(torch.exp(log_m).item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3wHrclzp11x"
      },
      "source": [
        "## Step 5 — Define the loss terms\n",
        "\n",
        "We combine three components:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\lambda_d \\mathcal{L}_{\\text{data}} + \\lambda_p \\mathcal{L}_{\\text{phys}} + \\lambda_{IC}\\mathcal{L}_{IC}\n",
        "$$\n",
        "\n",
        "where\n",
        "- **data loss:** fits measured $(x(t_i))$,\n",
        "- **physics loss:** enforces $(m \\ddot{x} + \\mu \\dot{x} + k x = 0)$,\n",
        "- **IC loss:** enforces $(x(0)=x_0, \\dot{x}(0)=v_0)$.\n",
        "\n",
        "---\n",
        "\n",
        "> 💡 **TODO:** Fill in the missing parts of the loss computation in the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO7TrLNoqPRY"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "lambda_d, lambda_p, lambda_ic = 1.0, 1.0, 10.0\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(list(model.parameters()) + [log_m], lr=lr)\n",
        "mse = nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46E6veFTqXd2",
        "outputId": "1523efab-f218-4532-8d4c-8722fe37b72c"
      },
      "outputs": [],
      "source": [
        "# 🔧 Training loop (students fill missing parts marked as TODO)\n",
        "num_epochs = 6000\n",
        "history = {\"m_est\": [], \"loss_total\": [], \"loss_data\": [], \"loss_phys\": [], \"loss_ic\": []}\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    model.train(); optimizer.zero_grad()\n",
        "\n",
        "    # --- supervised data loss ---\n",
        "    x_pred_data = model(t_train_t)\n",
        "    loss_data = mse(x_pred_data, x_train_t)\n",
        "\n",
        "    # physics residual on collocation\n",
        "    t_collocation_t.requires_grad_(True)\n",
        "    x_c = model(t_collocation_t)\n",
        "    x_t = time_derivative(x_c, t_collocation_t, order=1)\n",
        "    x_tt = time_derivative(x_c, t_collocation_t, order=2)\n",
        "\n",
        "    # current mass estimate (positive)\n",
        "    m_est = torch.exp(log_m)   # scalar tensor\n",
        "\n",
        "    # residual r = m x'' + mu x' + k x\n",
        "    residual = m_est * x_tt + mu * x_t + k * x_c\n",
        "    loss_phys = torch.mean(residual**2)\n",
        "\n",
        "    # --- physics residual ---\n",
        "    # TODO: compute x_c = ...\n",
        "    # TODO: compute first and second time derivatives\n",
        "    # TODO: compute residual = m_est * x_tt + mu * x_t + k * x_c\n",
        "    # HINT: use m_est = torch.exp(log_m)\n",
        "    # loss_phys = mean squared residual\n",
        "    # ===== YOUR CODE HERE =====\n",
        "    # x_c = ...\n",
        "    # x_t = ...\n",
        "    # x_tt = ...\n",
        "    # m_est = torch.exp(log_m)\n",
        "    # residual = ...\n",
        "    # loss_phys = torch.mean(residual**2)\n",
        "    # ==========================\n",
        "\n",
        "    # --- initial condition loss ---\n",
        "    x0_pred = model(t0_t)\n",
        "    v0_pred = time_derivative(x0_pred, t0_t, order=1)\n",
        "    loss_ic = mse(x0_pred, x0_t) + mse(v0_pred, v0_t)\n",
        "\n",
        "    # total loss\n",
        "    loss_total = lambda_d * loss_data + lambda_p * loss_phys + lambda_ic * loss_ic\n",
        "    loss_total.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- total loss ---\n",
        "    # TODO: combine into loss_total = ...\n",
        "    # loss_total = ...\n",
        "\n",
        "    # Backpropagation\n",
        "    # loss_total.backward()\n",
        "    # optimizer.step()\n",
        "\n",
        "    # Logging\n",
        "    with torch.no_grad():\n",
        "        history[\"m_est\"].append(float(torch.exp(log_m).item()))\n",
        "        #history[\"loss_total\"].append(loss_total.item())\n",
        "        #history[\"loss_data\"].append(loss_data.item())\n",
        "        #history[\"loss_phys\"].append(loss_phys.item())\n",
        "        #history[\"loss_ic\"].append(loss_ic.item())\n",
        "\n",
        "    # Optional: print every few hundred epochs\n",
        "    if (ep % 600 == 0) or (ep == num_epochs-1):\n",
        "        print(f\"Epoch {ep:5d} | m_est={float(torch.exp(log_m).item()):.4f}\")\n",
        "        #print(f\"Epoch {ep:5d} | total {loss_total.item():.4e} | data {loss_data.item():.4e} | phys {loss_phys.item():.4e} | ic {loss_ic.item():.4e} | m_est {m_est.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP_jqAfqqs_N"
      },
      "source": [
        "## Step 6 — Plot training progress\n",
        "\n",
        "We’ll plot:\n",
        "1. The learned trajectory $x_\\theta(t)$ compared with the data and ground truth.\n",
        "2. The inferred value of $m$ over training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBB3eTSjqkBC"
      },
      "outputs": [],
      "source": [
        "def plot_training_progress(history, t_all, x_all, model, log_m):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_pred = model(torch.tensor(t_all.reshape(-1,1), dtype=torch.float32, device=device)).cpu().numpy()\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(t_all, x_all, label=\"Ground truth\", linewidth=2)\n",
        "    plt.plot(t_all, x_pred, '--', label=\"PINN prediction\")\n",
        "    plt.scatter(t_train, x_train, color='tab:orange', label=\"data\")\n",
        "    plt.xlabel(\"t\"); plt.ylabel(\"x(t)\")\n",
        "    plt.legend(); plt.grid(alpha=0.3); plt.title(\"Trajectory\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history[\"m_est\"], label=\"estimated m\")\n",
        "    plt.axhline(m_true, color='black', linestyle='--', label=\"true m\")\n",
        "    plt.xlabel(\"epoch\"); plt.ylabel(\"m estimate\")\n",
        "    plt.legend(); plt.grid(alpha=0.3); plt.title(\"Parameter inference\")\n",
        "    plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "aR-u7euKq3Xq",
        "outputId": "fff87019-bc4c-4c75-9b0a-ea1c98c94e95"
      },
      "outputs": [],
      "source": [
        "# 🔍 After training (once students fill in the missing parts), run:\n",
        "plot_training_progress(history, t_all, x_all, model, log_m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCSUVtDjqozD"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "- How does the inferred mass $m$ evolve during training?\n",
        "- What happens if we change the number of data points or collocation points?\n",
        "- How sensitive is the estimation to the weighting of the losses?\n",
        "\n",
        "---\n",
        "\n",
        "**Goal:** Participants should fill in the missing parts of the physics loss and total loss definitions to make the PINN infer \\(m\\).\n",
        "\n",
        "---\n",
        "\n",
        "**Expected result:**  \n",
        "The PINN should converge to $m_\\text{est} \\approx 1.5$, and the predicted trajectory should closely match the ground truth beyond the data region.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "flux_partitioning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
