{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0hSETfuG2Cs"
      },
      "source": [
        "# Semi-parametric hybrid modelling — Q10 inference exercise\n",
        "\n",
        "**Goal:** estimate the Q10 parameter `Q10` in the respiration equation from data.\n",
        "\n",
        "At the end of the notebook export your estimated `Q10` and send it to Kai.\n",
        "\n",
        "---\n",
        "\n",
        "## Problem statement (Q10 formulation)\n",
        "\n",
        "We model ecosystem respiration as a multiplicative combination of a baseline dependence on covariates $R_b(X)$ and a temperature sensitivity factor $Q_{10}$:\n",
        "\n",
        "$$\n",
        "R_{\\text{eco}}(X, T_A) \\;=\\; R_b(X)\\ \\cdot\\ Q_{10}^{(T_A - T_A^{\\text{ref}})/10},\n",
        "$$\n",
        "\n",
        "where $X$ are covariates (may include $T_A$), $T_A$ is air temperature, and $T_A^{\\text{ref}} = 15$ (by convention here).  \n",
        "Our hybrid model is:\n",
        "\n",
        "$$\n",
        "\\hat y \\;=\\; \\underbrace{\\mathrm{f}_{a_\\theta}(X)}_{\\text{NN (learned factor)}} \\cdot \\underbrace{Q_{10}^{(T_A - 15)/10}}_{\\text{parametric }Q_{10}\\text{ factor}}\n",
        "$$\n",
        "\n",
        "We parameterize $Q10 = \\exp(\\alpha)$ and learn $\\alpha$ so $Q_{10}>0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install & imports (run first)\n",
        "!pip install -q torch torchvision matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb6CmpOVG85s",
        "outputId": "21260e6b-a3a4-4151-c38a-7e4a5ddb9030"
      },
      "outputs": [],
      "source": [
        "# Easy knobs for experiments\n",
        "CSV_PATH = \"./data/respiration.csv\"   # path to csv with columns x0,x1,x2,x3,y,y_obs,scenario\n",
        "SCENARIO_TRAIN = \"train\"\n",
        "SCENARIO_TEST = \"test\"\n",
        "TARGET_COL = \"y_obs\"   # prefer observed noisy column if present\n",
        "\n",
        "# Model / training knobs (participants can change these quickly)\n",
        "USE_TEMP_IN_NN = False        # equifinal (True) or identifiable (False)\n",
        "HIDDEN = 128\n",
        "NLAYERS = 3\n",
        "DROPOUT = 0.0                 # set e.g. 0.1 to enable dropout\n",
        "FINAL_POSITIVE = True         # softplus final activation\n",
        "INIT_Q10 = 2.5                 # initial guess for Q10 (exp(log_Q10))\n",
        "LR_NET = 5e-3\n",
        "LR_PARAM = 5e-4               # separate LR for log_Q10 (None to share)\n",
        "WEIGHT_DECAY = 0.0            # global weight decay on optimizer\n",
        "NUM_EPOCHS = 2000\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Regularization & diagnostics\n",
        "REG_PARAM_L2 = 0.0            # L2 penalty on (Q10 - Q10_prior)\n",
        "Q10_PRIOR = 2.0                # prior center for Q10 L2 penalty (if used)\n",
        "PRINT_EVERY = 300\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"DEVICE:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "T2oBa7Z6Ukq2",
        "outputId": "25b81865-332a-4c6d-f822-07719c614be9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- load CSV and basic validation ----\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "required_cols = {\"x0\",\"x1\",\"x2\",\"x3\",\"scenario\"}\n",
        "if not required_cols.issubset(df.columns):\n",
        "    raise ValueError(f\"CSV must contain columns: {required_cols}\")\n",
        "\n",
        "# pick target\n",
        "target_col = TARGET_COL\n",
        "\n",
        "# split by scenario column\n",
        "scenario = df[\"scenario\"].astype(str).values\n",
        "train_mask = scenario == SCENARIO_TRAIN\n",
        "test_mask  = scenario == SCENARIO_TEST\n",
        "\n",
        "X = df[[\"x0\",\"x1\",\"x2\",\"x3\"]].values.astype(float)\n",
        "y = df[target_col].values.astype(float).reshape(-1,1)\n",
        "\n",
        "X_train = X[train_mask]; y_train = y[train_mask]\n",
        "X_test  = X[test_mask];  y_test  = y[test_mask]\n",
        "\n",
        "print(\"Data sizes — total:\", X.shape[0], \"train:\", X_train.shape[0], \"test:\", X_test.shape[0])\n",
        "\n",
        "# quick plot: y vs x0\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.scatter(X_train[:,0], y_train, label=\"train\", alpha=0.6)\n",
        "plt.scatter(X_test[:,0], y_test, label=\"test\", alpha=0.6)\n",
        "plt.xlabel(\"x0 (T_A)\"); plt.ylabel(\"y (respiration)\")\n",
        "plt.title(\"Data overview\")\n",
        "plt.legend(); plt.grid(alpha=0.3); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-r50R4OVL59"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, n_inputs=4, hidden=64, nlayers=3, dropout=0.0,\n",
        "                 final_positive=True, use_temp_in_nn=True, init_Q10=2.0):\n",
        "        \"\"\"\n",
        "        - use_temp_in_nn: if True NN receives x0 as input; if False NN receives only x[:,1:].\n",
        "        - dropout: probability for nn.Dropout (0 disables).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.use_temp_in_nn = use_temp_in_nn\n",
        "        nn_input = n_inputs if use_temp_in_nn else (n_inputs - 1)\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(nn_input, hidden))\n",
        "        layers.append(nn.Tanh())\n",
        "        if dropout > 0:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        for _ in range(nlayers-1):\n",
        "            layers.append(nn.Linear(hidden, hidden))\n",
        "            layers.append(nn.Tanh())\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "        layers.append(nn.Linear(hidden, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.final_positive = final_positive\n",
        "        # trainable log Q10\n",
        "        self.log_Q10 = nn.Parameter(torch.tensor(np.log(init_Q10), dtype=torch.float32))\n",
        "        # init\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N,4)\n",
        "        nn_in = x if self.use_temp_in_nn else x[:,1:]\n",
        "        fa = self.net(nn_in)\n",
        "        if self.final_positive:\n",
        "            fa = F.softplus(fa) + 1e-8\n",
        "        Q10 = torch.exp(self.log_Q10)   # scalar\n",
        "        x0 = x[:,0:1]\n",
        "        fp = Q10 ** ((x0 - 15.0) / 10.0)\n",
        "        yhat = fa * fp\n",
        "        return yhat, fa, Q10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1OhkdNxHLI1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def make_loaders(X_train, y_train, X_test, y_test, batch_size):\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
        "    y_train_t = torch.tensor(y_train, dtype=torch.float32).to(DEVICE)\n",
        "    X_test_t  = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
        "    y_test_t  = torch.tensor(y_test, dtype=torch.float32).to(DEVICE)\n",
        "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(TensorDataset(X_test_t, y_test_t),  batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader, X_train_t, y_train_t, X_test_t, y_test_t\n",
        "\n",
        "def train_hybrid(model, train_loader, val_loader=None, lr_net=1e-3, lr_param=None,\n",
        "                 weight_decay=0.0, num_epochs=1000, reg_param_l2=0.0, Q10_prior=1.5, print_every=100):\n",
        "    model = model.to(DEVICE)\n",
        "    # param groups\n",
        "    if hasattr(model, \"log_Q10\") and lr_param is not None:\n",
        "        net_params = [p for n,p in model.named_parameters() if n != \"log_Q10\"]\n",
        "        optimizer = optim.Adam([{'params': net_params, 'lr': lr_net},\n",
        "                                {'params': [model.log_Q10], 'lr': lr_param}], weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr_net, weight_decay=weight_decay)\n",
        "    criterion = nn.MSELoss()\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"Q10\": []}\n",
        "\n",
        "    for ep in range(1, num_epochs+1):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            yhat, fa, Q10 = model(xb)\n",
        "            loss = criterion(yhat, yb)\n",
        "            if reg_param_l2 > 0.0:\n",
        "                loss = loss + reg_param_l2 * ( (Q10 - Q10_prior)**2 )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(float(loss.item()))\n",
        "        # epoch end: evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # train\n",
        "            train_losses = []\n",
        "            for xb, yb in train_loader:\n",
        "                yhat, _, _ = model(xb)\n",
        "                train_losses.append(float(((yhat - yb)**2).mean().item()))\n",
        "            train_loss = float(np.mean(train_losses))\n",
        "            # val\n",
        "            if val_loader is not None:\n",
        "                val_losses = []\n",
        "                for xb, yb in val_loader:\n",
        "                    yhat, _, _ = model(xb)\n",
        "                    val_losses.append(float(((yhat - yb)**2).mean().item()))\n",
        "                val_loss = float(np.mean(val_losses))\n",
        "            else:\n",
        "                val_loss = None\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"val_loss\"].append(val_loss)\n",
        "            history[\"Q10\"].append(float(torch.exp(model.log_Q10).cpu().numpy()))\n",
        "        if ep % print_every == 0 or ep==1 or ep==num_epochs:\n",
        "            print(f\"Epoch {ep:4d} | train {train_loss:.4e} | val {val_loss} | Q10 {history['Q10'][-1]:.4f}\")\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "uR09BNfOHWbf",
        "outputId": "c9844f32-5245-4da3-c66f-7d3776b83229"
      },
      "outputs": [],
      "source": [
        "# prepare loaders\n",
        "train_loader, test_loader, X_train_t, y_train_t, X_test_t, y_test_t = make_loaders(X_train, y_train, X_test, y_test, BATCH_SIZE)\n",
        "\n",
        "#instantiate model with knobs\n",
        "model = HybridModel(n_inputs=4, hidden=HIDDEN, nlayers=NLAYERS, dropout=DROPOUT,\n",
        "                    final_positive=FINAL_POSITIVE, use_temp_in_nn=USE_TEMP_IN_NN, init_Q10=INIT_Q10).to(DEVICE)\n",
        "\n",
        "# train\n",
        "model, history = train_hybrid(model, train_loader, val_loader=test_loader,\n",
        "                              lr_net=LR_NET, lr_param=LR_PARAM, weight_decay=WEIGHT_DECAY,\n",
        "                              num_epochs=NUM_EPOCHS, reg_param_l2=REG_PARAM_L2, Q10_prior=Q10_PRIOR,\n",
        "                              print_every=PRINT_EVERY)\n",
        "\n",
        "# plotting helper\n",
        "def plot_results(history, Q10_true=None):\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(history[\"train_loss\"], label='Train Loss')\n",
        "    plt.plot(history[\"val_loss\"], label='Validation Loss')\n",
        "    plt.xlabel('epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(alpha=0.2)\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(history[\"Q10\"], label='Q10 trace')\n",
        "    if Q10_true is not None:\n",
        "        plt.axhline(Q10_true, color='k', linestyle='--', label='true Q10')\n",
        "    plt.xlabel('epoch'); plt.ylabel('Q10'); plt.legend(); plt.grid(alpha=0.2)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# plot\n",
        "plot_results(history, Q10_true=None)\n",
        "\n",
        "# print final Q10\n",
        "final_Q10 = float(torch.exp(model.log_Q10).detach().cpu().numpy())\n",
        "print(\"Final learned Q10:\", final_Q10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zRYxsa7oD8Q"
      },
      "source": [
        "## Optional things to play with (make them easy to change at top):\n",
        "- **capacity of the network**: change `HIDDEN` and `NLAYERS`.\n",
        "- **learning rates**: change `LR_NET` and `LR_PARAM`.\n",
        "- **with or without `T_A` (`x0`) in the NN**: set `USE_TEMP_IN_NN = True/False`.\n",
        "- **regularization**: change `WEIGHT_DECAY` (optimizer weight decay).\n",
        "- **dropout**: set `DROPOUT = 0.1` (or 0.2) to enable Dropout in the NN.\n",
        "- **initial value**: change `INIT_Q10` (initial guess).\n",
        "- **Q10 prior**: can you anchor the estimations in the equifinal case (with `T_A` in NN) through a prior on the Q10? Can the NN compensate bad estimate?\n",
        "\n",
        "Try a few combinations and compare the final `Q10` estimates. Send your estimations and the validation performance to Kai.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "flux_partitioning_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
